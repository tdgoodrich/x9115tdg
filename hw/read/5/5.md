# READ5

## Paper reference:

Gary Wassermann, Dachuan Yu, Ajay Chander, Dinakar Dhurjati, Hiroshi Inamura, and Zhendong Su. 2008. Dynamic test input generation for web applications. In Proceedings of the 2008 international symposium on Software testing and analysis (ISSTA '08). ACM, New York, NY, USA, 249-260.

The paper had 143 citations at time of writing.

## Keywords:

* ii1: **Automated web application testing**: Automatically testing web applications involve both static and dynamic analysis, with an emphasis on the latter. Dynamic testing is computationally difficult given the large number of input options for a web page, making it an active area of current research.
* ii2: **Dynamic test generation**: Dynamic testing gauges how well a piece of software responds to input, and generating sufficient tests is difficult (especially for web pages). Generating such tests is an active area of research. 
* ii3: **Test coverage**: The coverage of a test measures how many branches (paths) a test covers in a piece of software. Different measures exist, such as lines covered, function calls covered, etc.
* ii4: **Concolic testing framework**: Concolic (concrete + symbolic) testing is a hybrid technique for performing both symbolic execution (“theoretical”) and concrete execution (“practical”) with real inputs. 

## Notes:

* iii1: Motivation: The authors of this paper provide more compelling arguments than the papers in READ2-4 for why web application testing is hard, and the current tools do not apply, compared to C- or Java-style testing. Specifically, the identify the following (paraphrased) issues:
    1. PHP is a scripting language, allows arbitrary meta-programming and more string- and array-centric programming than in C or Java.
    2. Automated test generation needs test oracles, and existing ones are build for memory or null pointer errors (problems that PHP does not have).
    3. Finally, most existing concolic testing is made for unit tests, whereas automated tests need these tools on a much larger scale. 
* iii2: Tutorial: The authors make their work very approachable by including a short tutorial, including sample PHP code, the finite state machines created for symbolic execution, and the context-free language for expressions (the concrete execution stage). Finally, they provide full pseudocode for their algorithm, along with some pre-processing scripts. 
* iii3: Data: The authors detail three datasets used:
    1. Mantis 1.0.0rc2, an open source bug tracking system with a SQL injection vulnerability in the “lost password” page. This dataset is composed of 17,328 lines of PHP.
    2. Mambo 4.5.3, an open source content management system with a SQL injection vulnerability in the “submit weblink” page. This dataset is composed of 13,248 lines of PHP.
    3. Utopia News Pro 1.3.0, a news management system with a SQL injection vulnerability due to insufficient regular expression filtering in the user-management page. This dataset is composed of 1,528 lines of PHP. 
Interestingly, these datasets are not used in future work, notably the papers in READ1 and READ2.
* iii4: Related Work: The authors do a nice job of outlining existing dynamic testing work, which is mostly limited to random test generation. The work that had been done with concolic testing was limited by the problems in the motivation (see iii1). That is, these tests were written for C- and Java-based software with different vulnerabilities and errors. Static testing work for web pages is much more in depth, yet not quite relevant given that the authors are trying to perform dynamic analysis of PHP web applications. 

## Improvements:

* iv1: No research questions. Part of what gave READ1-4 such good structure was the inclusion of 3-4 specific and driving research questions that guided either the survey (READ3) or the experiments (READ1/2/4). These questions not only provided the motivation for why choices were made, but also gave the reader a lens for interpreting the results and how they are useful in the bigger picture. 
* iv2: More statistics. The reported statistics were run times and maximum output file sizes, without any details on how many times the experiment was run or what the distribution looks like. These could easily have been included in the same space and provided much more information! 
* iv3: More varied datasets. The three datasets the authors use all contain SQL injection vulnerabilities, and yet the authors claim to be testing dynamic web application in general. Their analysis would be more well-rounded with a more heterogeneous corpus of datasets, perhaps some with DoS vulnerabilities or input errors less severe than a full SQL injection attack.