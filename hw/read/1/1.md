# READ1 

## Paper reference:

Nadia Alshahwan and Mark Harman. 2011. Automated web application testing using search based software engineering. In Proceedings of the 2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE '11). IEEE Computer Society, Washington, DC, USA, 3-12.

This paper had 57 citations at time of writing.

## Keywords:

* ii1: **Search-Based Software Engineering (SBSE)**: Applying metaheuristic search techniques (gradient-ascending, simulated annealing, etc.) to perform typical software engineering tasks (generating software tests, optimizing parameters, etc.).
* ii2: **Automated web application testing**: Automatically testing web applications involve both static and dynamic analysis, with an emphasis on the latter. Dynamic testing is computationally difficult given the large number of input options for a web page, making it an active area of current research.
* ii3: **Static analysis**: Static analysis for software testing is where the code is tested without being run (verification). Typical tests include data flow charts and proofreading code.  
* ii4: **Dynamic analysis**: Dynamic analysis for software testing is where the code is tested by being run (validation). Typical tests include checking how user input is handled, and gauging run times and memory usage. 

## Notes:

* iii1: Patterns: The authors include a useful section entitled Issues and Solutions in Web Application Testing, where they address different issues specific to web applications. For example:  
**Issue**: *Dynamic Typing*  
**Description**: *Web development languages such as PHP, Python and Ruby are dynamically typed. All variables are initially treated as strings. If used in an arithmetic expression, they are treated as numeric at that operation. However, the same input can be treated as numeric in one expression and as a string in a different expression within the same script. This makes it hard to decide the type of variables involved in a predicate, posing a problem when deciding which fitness function to use.*  
**Solution**: *To solve this problem, types of variables are checked dynamically at run-time using built-in PHP functions and then directed to the appropriate fitness function.*

* iii2: Related Work: The paper concludes with a nice section on related work and how the authors’ work is different. Specifically, they note that SBSE has been applied broadly to functional and non-functional works before, but very few works exist for automated test generation for web applications; only Marchetto and Tonella’s work with Ajax testing using Hill Climbing is cited.  
* iii3: New Results: In response to the lack of related work, the authors develop a new search-based testing technique based on “Dynamically Mined Values” (a mechanism for seeding the generation of new input with the web application’s response from the current input. The authors note that their work builds on an existing method (Alternating Variable Method by Korel) and a systematic technique based on Michael et al, but is not limited to the specific applications that these works applied to. 
* iii4: Data: The authors note that “We took steps to insure that our results would be reproducible. The state of the application was initialized before each test case is called. The applications used are open source and thus publically available. Bug reports are available online http://www.cs.ucl.ac.uk/staff/nalshahw/swat .”


## Improvements:
* iv1: Now what? While the authors make efforts to provide reproducible experiments, they do not formulate any concrete future directions or problems in their conclusions. 
* iv2: Need for synthetic data. A severe limitation of the experiment performed is that it does not meet the authors’ goal of developing a fully automatic tool. That is, the data needed the user to input data types, user names and passwords, etc. One improvement would have been if the authors could have generated a synthetic data set (based on the real-world data sets they provided), allowing their tool and future tools to automatically generate full tests. The worry here is that the authors might be underestimating the amount of work that the user does manually, and are developing a semi-automated tool that cannot possible be fully automated.
* iv3: Need for more data. In the experimental setup the authors note that they run 3 versions of their tool 30 times on 6 PHP applications. The input corpus seems small, both in number and in variety. Again, if real-world data is difficult to obtain, then at least synthetic data should be included as a point of comparison. 