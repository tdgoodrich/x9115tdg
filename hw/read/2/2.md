# READ2

## Paper reference:

Shay Artzi, Adam Kiezun, Julian Dolby, Frank Tip, Daniel Dig, Amit Paradkar, and Michael D. Ernst. 2010. Finding Bugs in Web Applications Using Dynamic Test Generation and Explicit-State Model Checking. IEEE Trans. Softw. Eng. 36, 4 (July 2010), 474-494.

This paper had 100 citations at time of writing.

## Keywords:

* ii1: **Program testing**: Performing an investigation to gauge the quality of a piece of software, especially when used under specific conditions.
* ii2: **Program verification**: Program verification checks that the program was written correctly, and is typically associated with static testing.
* ii3: **Dynamic test generation**: Dynamic testing gauges how well a piece of software responds to input, and generating sufficient tests is difficult (especially for web pages). Generating such tests is an active area of research. 
* ii4: **Explicit state model checking**: Model checking involves testing whether a piece of software meets its specification. Explicit state model checking walks through every possible state explicitly, and typically has exponential run time. 

## Notes:

* iii1: Data: This paper seems to introduce to the literature the 6-PHP-program input corpus that is used in the 2011 paper I selected (see READ1). The projects (faqforge, webchess, schoolmate, phpsysinfo, timeclock, phpBB2) are open source and taken from http://sourceforge.net. 
* iii2: Baseline Results: The authors note that there does not exist an automated method for generating tests for dynamic web pages, and yet “PHP [powers] 21 million domains as of April 2007.” To solve this problem, the authors set a baseline for dynamically discovering web page input, handling data types, branching smartly based on dynamic output, and implementing a tool (Apollo) that handles all these tasks. In their result section, the authors provide their baseline statistics for things like line coverage %, execution crashes/errors/warnings, total faults, etc. In total, the authors provide a firm starting point for further research. 
* iii3: Tutorial Materials: To fully illustrate the difficulty of this problem, the authors include a sizable walkthrough (11 of 21 pages) of a full scenario. The start by defining possible failures in PHP programs, with example code and a bulleted list of problems. They then show how a naïve algorithm would execute and miss several of the bugs, and then define their Apollo algorithm and how it applies to this example. Optimizations (such as path constrain minimization) are also included in the example. 
* iii4: Checklist: To guide the reader through the analysis, the authors include a checklist of research questions they hope to answer with their experiments:
    * Q1. How many faults can Apollo find, and of what varieties? 
    * Q2. How effective is the fault detection technique of Apollo compared to alternative approaches in terms of the number and severity of discovered faults and the line coverage achieved? 
    * Q3. How effective is our minimization technique in reducing the size of input parameter constraints and failure-inducing inputs? 


## Improvements:

* iv1: Justification of data set. This paper does not cite previous usage of these PHP programs as an input corpus, nor do the authors justify their selection of these specific projects. Considering that this data set has been reused in the literature (see READ1) and seems to be the precedent, it is a bit troublesome that no explanation is given for this particular selection. 
* iv2: Graphics choice. The paper is very well written and generally complete, but the graphics are terribly low-resolution and blurry. Perhaps this is knit-picking, but it stands out as very low-quality. 
* iv3: Lack of competition/simple baseline. While the paper does establish a baseline research niche, the authors do not spend much time establishing what a good baseline result looks like. They compare their (complicated) Apollo implementation to a (dumb) randomized algorithm. The problem is that the options are complicated-and-good vs. simple-and-dumb. What about simple-and-a-little-clever? That way Apollo has some real competition, and if it does better than a smart baseline then the authors have a real result. As it stands, neither algorithm makes for a good comparison, which we see later on when the paper in READ1 does not compare directly to Apollo. 